---
title: "Parsing Text using Dictionaries"
author: "Colin Fay and Julia Silge"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Parsing Text using Dictionaries}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## What can a dictionary-based approach offer for non-English languages?

Rules for delimiting between words vary from one language to another. In French, an apostrophe is used as a delimiter between an article and a noun when this noun starts with a vowel. For exemple, "l'arriere" is considered two words in French: "le + arriere" contracted to "l'arriere". 

If we tokenize French with many tokenization approaches including what is implemented in [tokenizers](https://github.com/ropensci/tokenizers), the split is not made on the apostrophe. 

```{r}
library(dplyr)
library(tidytext)

df_fr <- tibble(text = c("C'est l'arriere-cuisine.", "On s'y trouve."))
unnest_tokens(df_fr, word, text)
```

Here, we end up with six tokens where we should have nine, if we want to count the contracted words separately. 

## Using tidytext with dictionary-based word identifications 

Instead of straightforward tokenization, we can transform our text into a tidy format using a dictionary-based approach. This is implemented using [`hunspell` dictionaries and the `hunspell` parser](https://github.com/ropensci/hunspell).

### With included dictionaries

You can get the list of dictionaries available in tidytext with `supported_languages()`. 

```{r}
supported_dictionaries()
```

These `hunspell` dictionaries are available in the `extdata` folder of the tidytext package. 

To transform text to a tidy data frame using one of these dictionaries, use the `dict` argument in the `unnest_tokens()` function. 

```{r}
unnest_tokens(df_fr, word, text, dict = "french") 
```

Here, we can see the split is done with the apostrophe, giving nine tokens. This approach is only available for tokenizing by single words.

You can also unnest other formats, such as HTML.

```{r}
data_frame(row = 1:2,
           text = c("<h1>Du text <b>d'abord</b>", "<a href='example.com'>C'est l'ancien site</a>")) %>%
  unnest_tokens(word, text, format = "html", dict = "french")
```


### With your own dictionnary

You can pass your own `hunspell` dictionary to `unnest_tokens()` by specifying a path, `dict = "path/to/your/dict`.

```{r}
df_ice <- tibble(text = "Eitt tungumál er aldrei nóg")
my_dict <- system.file("extdata/icelandic.aff", package = "tidytext")

unnest_tokens(df_ice, word, text, dict = my_dict) 
```

